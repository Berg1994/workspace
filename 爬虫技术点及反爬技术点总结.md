[TOC]

## 爬虫技术点及反爬技术点总结

####选择Python做爬虫的理由

```
C，C++。高效率，快速，适合通用搜索引擎做全网爬取。缺点，开发慢，写起来又臭又长，例如：天网搜索源代码。

选择Python
跨平台，对Linux和windows都有不错的支持
科学计算，数值拟合：Numpy，Scipy

```

#### 爬取前的反爬分析

```
1.判断反爬方向
逻辑： 
循环计数爬取首页  计算反爬计数阈值
增设代理IP  继续循环爬取 查看是否被封 没封 判断反爬为IP 被封 继续增设
设置
```



### 生疏技术点记录

```python

```

#### 字符串去除空白

```python
' '.join(str1.split())

list2 = ''.join(list1)
list3 = re.sub(r'\n|\r|\t','',list2)

s = "I   LOVE  YOU"
>>> re.sub(" +", " ", s)
'I LOVE YOU'
```



#### xpath 方法使用

```python
#xpath 类无法通过一个类属性定位  通过方法包含字符串 即可使用
//div[contains(@class,"post-body")]//p/text()
```



#### 生成哈希摘要

```python
1 import hashlib
2 
3 data =  'This a md5 test!'
4 hash_md5 = hashlib.md5(data)
5 
6 hash_md5.hexdigest()
```



#### scrapy_shell使用

```markdown
scrapy shell
    是什么？是一个scrapy的调试工具，用它来进行调试xpath，在scrapy shell中测试的是对的，到代码中肯定是对的。
    ipython : pip install ipython
    使用方式：scrapy shell url
    常用方法 
    response.xpath() 查询标签
```



#### 设置scrapy_log日志打印

```python
# 配置错误等级
LOG_LEVEL = 'DEBUG'
LOG_FILE = 'log.txt'
```



#### 确认爬虫进度

```
打印
日志
```

#### 增量爬取和去重

```
update

```

#### 并行并发

```python
某个系统支持两个或者多个动作（Action）同时存在，那么这个系统就是一个并发系统。
某个系统支持两个或者多个动作同时执行，那么这个系统就是一个并行系统。

并发和并行都可以是多线程，看这些线程能否同时被（多个）cpu执行，如果可以则说明是并行，而并发是多个线程被（一个）cpu轮流切换执行

```







#### 证书取消（`ssl`）

```python
from requests.packages import urllib3
#忽略警告
urllib3.disable_warnings()
r = requests.get(url=url, headers=headers, verify=False)


#设置全局的取消证书验证
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

```

#### 识别网站技术

```python
import builtwith
res = builtwith.parse('https://www.jianshu.com/', headers=headers)
"""
返回实例 
{'web-frameworks': ['Twitter Bootstrap', 'Ruby on Rails'], 'programming-languages': ['Ruby']}
"""
```



#### `headless`

```python
from selenium.webdriver.chrome.options import Options
	chrome_options = Options()
	chrome_options.add_argument('--headless')
    #设置图形显示
	chrome_options.add_argument('--disable-gpu')
    #设置代理
	chome_options.add_argument(('--proxy-server=http://' + ip))
    #打开页面
    browser = webdriver.Chrome(chrome_options=chrome_options)
    browser.get(...)
    #保存截图
    browser.save_screenshot('....jpg')
```

#### 请求次数及随机延时

```python
class Retry(object):

    def __init__(self, *, retry_times=3,
                 wait_secs=5, errors=(Exception,)):
        self.retry_times = retry_times
        self.wait_secs = wait_secs
        self.errors = errors

    def __call__(self, fn):

        def wrapper(*args, **kwargs):
            for _ in range(self.retry_times):
                try:
                    return fn(*args, **kwargs)
                except self.errors as e:
                    print(e)
                    sleep((random() + 1) * self.wait_secs)
            return None

        return wrapper
```



### 爬取难点总结

#### 获取最后一页

```
常见办法 
1.判断是否有没有最后一页
2.查找max 值
3.获取页面类 如div[@class='pagenavi']/a[last()-1] 
通过分析页面获取pagenavi类属性  查找下面的a标签即可获取所有标签
通过[last()] 方法可以获取最后一个a标签  -1 则获取倒数第二个 此法更加具体情况使用
```



#### 懒加载

```python
#常见懒加载标志 src2   data-src   data-original  class="lazy"

```



#### 数据隐藏位置(如`js`,`XHR`)

#### 接口提取需要的键值方式（`jsonpath`）



###数据持久化方式

```python

```

#### mongodb给字段添加子级字段

```python
    def process_item(self, item, spider):
        if isinstance(item, UserItem):
            self.collections.update({'id': item['id']}, {'$set': item}, True)
        if isinstance(item, UserRelationItem):
            """给微博用户添加粉丝，关注字段"""
            self.collections.update(
                {'id': item['id']},
                {'$addToSet': {
                    'fans': {'$each': item['fans']},
                    'follower': {'$each': item['follower']}
                    }
                }
            )
        return item
```



#### scrapy_键值对形式处理字符串

```python
   def process_item(self, item, spider):
        # 将item写到文件中
        # 将item转化为字典
        obj = dict(item)
        string = json.dumps(obj, ensure_ascii=False)
        self.fp.write(string + '\n')
        return item
```



#### scrapy_图片存储

```python
#Scrapy用ImagesPipeline类提供一种方便的方式来下载和存储图片
"""
具有以下特点:
1.将下载图片转换成通用JPG和RGB格式
2.避免重复下载
3.缩略图生成
4.图片大小过滤
"""
#工作流程
"""

从spider爬取的项目图片URL放入image_urls列表中，
返回item到pipeLine
当进去Imagepipline， image_urls列表中的URLs将被Scrapy的调度器和下载器安排下载（这意味着调度器和中间件可以复用），当优先级越高，会在其他页面被抓取前处理。
项目会在这个特定管道阶段保持“locker”状态，直到完成图片的下载或者引发错误
当图片下载完成，另一个组（images）将被更新到结构中，这个组包含一个字段列表，包括下载图片的信息，如下载路径，源抓取地址（从image_urls组获得）和图片的校验码。
images列表中的图片顺序将和原image_urls组保持一致，如果某个图片下载失败，将会记录下错误信息，图片也不会出现在images组中
"""

#实现方法
在Items.py中添加实体类 并写好字段
#以妹子图为例
import scrapy

class MeiziItem(scrapy.Item):
    #爬取的图集名
    name = scrapy.Field()
    #爬取的图片链接
    images_urls = scrapy.Field()
    #爬取的每页链接
    url = scrapy.Field()
    
    
#Spider中操作
'''
主要区别在于 先声明一个类属性img_urls = []
在爬取到每个图集的链接后  通过一个成员方法将所有同图集的图片链接返回给管道
'''
#中间件设置
class MeiZiTu(object):
    #每次发送请求到下载页面时使用
    def process_request(self, request, spider):
        #给请求添加referer属性 如果有， 没有就设置成None
        referer = request.meta.get('referer', None)
        if referer:
            request.headers['referer'] = referer
    
#在MeiziPipeline中实现数据清理下载等功能  该类继承ImagesPipline类
"""
继承类需要复写4个方法  
get_media_requests 用于进行图片下载请求处理
item_completed 用于图片下载完成的通知
file_path 实现下载原始图片的保存路径
thumb_path 用于缩略图保存路径
"""

class MeiziPipline(ImagesPipline):
    def file_path(self, request, response=None, info=None):
        """
        设置文件路径
        """
        item = request.meta['item']
        folder = item['name']
        folder_strip = strip(folder)
        image_guid = request.url.split('/')[-1]
        filename = u'full/{0}/{1}'.format(folder_strip, image_guid)
        return filename

    def get_meida_requests(self, item, info):
        #重写方法 并对各个图片返回一个Request对象 通过管道把图片在给调度器下载器下载
		for img_url in item['images_urls']:
            referfer = item['url']
            #返回的请求需要带上防盗链接 这是妹子网的反爬措施
            yield Request(img_url, meta={'item':item, 'referer':referer})
            
     def item_completed(self, results, item, info):
        #对于下载完成的图片进行判断
        image_paths = [x['path'] for ok, x in results if ok]
        if not image_paths:
            raise DropItem('Item contains no images')
        return item
    
    def thumb_path(self, request, thumb_id, response=None, info=None):
        item = request.meta['item']
        image_guid = request.url.split('/')[-1]
        #thumb_id 就是在setting中定义的big,small
        filenames = 'thumbil/%s/%s/%s' % (thumb_id, item['name'], image_guid)
		return filenames
    
        
        
#在settings中设置路径时效等参数
# 设置保存路径
IMAGES_STORE = 'D:\codeingSpace\pythoncode\meizi\image\\'
# 设置时效
IMAGES_EXPIRES = 30

#设置缩略图的大小 2种方式
IMAGES_THUMBS = {
    'small':(50, 50),
    'big': (270, 270),
}
#设置图片（宽最小、高最小）过滤，用于方式爬取到如广告等无用图
IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110

#常见设定
#减少超时时间以提高爬取速度
DOWNLOAD_TIMEOUT = 15

#不对失败的HTTP请求进行重试
RETRY_ENABLED = False

#禁用cookie
COOKIES_ENABLED = False


        
            


```



#### 文本储存

```
通过-o参数来指定文件名，Scrapy支持我们将爬取到的数据导出成JSON、CSV、XML、pickle、marshal等格式。
scrapy crawl qiubai -o qiubai.json
scrapy crawl qiubai -o qiubai.xml
scrapy crawl qiubai -o qiubai.csv

解决输出csv有空行问题
        https://blog.csdn.net/qq_38282706/article/details/80279912
```

#### 保存方式

```python
#通过写入文件
response = urllib.request.urlopen(url)

with open('meinv.jpg', 'wb') as fp:
	fp.write(response.read())
#通过urlretrieve()方法
urllib.request.urlretrieve(url, 'meinv2.png')
```



#### 常用去重手段

```
1.爬取的url 链接可以放在set()里面
2.
```



#### 对于字段不完整数据报错处理

```python
   #有些网址数据不完全是固定格式，当爬取不到数据就会报错  所以应当自己填充
    try:
        down_total = soup.select('.bus_line_no')[1].string
        # 下行站牌
        div1 = soup.select('.bus_site_layer')[1]
        down_href_list = div1.select('div > a')
        down_name_list = []
        for oa in down_href_list:
            down_name_list.append(oa.string)
    except Exception as e:
        down_total = '亲-没有下行'
        down_name_list = '亲-这是环形'
```



### 常用数据转换保存思路

#### 淘宝评价保存

```python
#淘宝评价获取的文本是字符串 所以先拼接成字典格式
content = urllib.request.urlopen(request).read().decode('gbk')
content = '{' + content + '}'
#再通过json转成一个json对象 即键值形式
obj = json.loads(content)
#再从对象中爬取字段
comments_list = obj['rateDetail']['rateList']
# 遍历列表，依次获取每一个评论内容
for com_obj in comments_list:
    # 评论内容
    comment = com_obj['rateContent']
#最后把字段写成字典形式转字符串保存
 item = {
        '评论内容': comment,
        '用户名': name,
        '照片': images,
        '评论时间': tt
    }
    fp.write(str(item) + '\n')
    
"""
保存结果实例
{'评论内容': '用了一段时间才来评价的，首先确认是正品，产品质量有保证，使用顺手流畅，显示清晰细腻，功能强大。一如既往的好一如既往的喜欢。', '用户名': '踏***8', '照片': '', '评论时间': '2018-08-09 23:20:06'}
"""
```

### 传统反爬总结

1. 设置单个IP访问阈值
2. 单个session访问阈值
3. 单个user_agent访问阈值
4. 检查HTTP请求的Headers信息，包括User-Agent, Referer、Cookies等
5. 以上组合

### 反反爬总结

```
user_agent 设置
在scrapy中设置中间键
```

#### 设置随机user_agent

```python
ua = UserAgent()
headers = {'User-Agent': ua.random}
```



### 模块生疏点

####`scrapy_redis`在settings中的配置

```python
#SCHEDULER 是任务分发与调度，把所有的爬虫开始的请求都放在redis里面，所有爬虫都去redis里面读取请求

#DUPEFLTER_CLASS 是去重队列，负责所有请求的去重，
#REDIS_START_URLS_AS_SET 指的是使用redis里面的set类型（简单完成去重），如果你没有设置，默认会选用list.

#
```

### 实用技巧

### item快速赋值

```python
#item赋值一个一个修改起来非常蠢  通过eval（）方法 再加上一个列表循环 可以快速赋值
def parse(self, response):
        odiv_list = response.xpath('//div[@id="content-left"]/div')

        for odiv in odiv_list:
            # 创建对象
            item = QiubaiproItem()
            # 获取头像
            face = odiv.xpath('./div[@class="author clearfix"]//img/@src')[0].extract()
            # 获取用户名
            name = odiv.xpath('./div[@class="author clearfix"]//h2/text()')[0].extract().strip('\n')
            # 获取年龄
            try:
                age = odiv.xpath('./div[@class="author clearfix"]/div/text()')[0].extract()
            except Exception as e:
                age = '没有年龄'
            # 获取内容
            content_lt = odiv.xpath('.//div[@class="content"]/span[1]/text()').extract()
            content = ''.join(content_lt).strip('\n ')
            # 好笑个数
            haha_count = odiv.xpath('.//span[@class="stats-vote"]/i/text()')[0].extract()
            
            # 将抓取得到的内容依次保存到item里面
            # item['face'] = face
            # item['name'] = name
            # item['age'] = age
            # item['content'] = content
            # item['haha_count'] = haha_count
            lt = ['face', 'name', 'age', 'content', 'haha_count']
            for field in lt:
                item[field] = eval(field)
            
            yield item
```

## 部署上线问题

```python
#1. 在Centos 下 同时安装了python2,3  没有配置环境变量echo $PATH  就算建立了pip3 软链接  安装的包的使用命令也无法找到 使用命令 如
python3 -m -pip install virtualenv 
#2. 加载linux 下的虚拟环境 需要使用
source activate
#3.在建立好的虚拟环境下 可以直接使用python3下 安装的包命令 如 scrapyd

#4.如果在liunx 下没有配置环境变量  可以通过
"""
〜/ .bash_profile中中添加export PATH=$PATH:/usr/local/bin（假设PIP的安装目录为在/ usr /本地/ bin）中然后source ~/.bash_profile使之生效
"""

    

```

